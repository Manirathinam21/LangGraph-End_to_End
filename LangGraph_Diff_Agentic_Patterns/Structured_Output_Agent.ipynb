{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Structured Output Agent in LangChain (or other LLM frameworks) refers to an agent that produces responses in a specific structured format, such as JSON, dictionaries, or other structured data types. These agents are particularly useful when you want to ensure that your responses conform to a defined schema, making it easier to parse, validate, and utilize in downstream applications.**\n",
    "\n",
    "Why Use a Structured Output Agent?\n",
    "- Controlled Responses: Ensures that the outputs follow a specific format, reducing ambiguity.\n",
    "- Data Validation: Useful when you want the model output to fit predefined structures, like for APIs or database entries.\n",
    "- Automation: Makes integration with systems like dashboards, ETL pipelines, or business workflows easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(model='gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tool with tavily_search\n",
    "\n",
    "@tool\n",
    "def call_tool(prompt:str):\n",
    "    \"\"\"Should do a web search to find the required city details\"\"\"\n",
    "    search= DuckDuckGoSearchRun()\n",
    "    response= search.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "#search= TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "#search= TavilySearchResults()\n",
    "tools= [call_tool]\n",
    "\n",
    "llm_with_tools= llm.bind_tools(tools)\n",
    "\n",
    "tool_node= ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The BaseModel class is a core component of Pydantic, which is used extensively in Python applications for data validation, type checking, and parsing. It's particularly useful when working with APIs, web frameworks like FastAPI, and other scenarios where you need to enforce data structure and ensure data integrity.**\n",
    "\n",
    "**Here's an overview of the BaseModel class, along with some examples to demonstrate how you can use it effectively.**\n",
    "\n",
    "**This code defines a Pydantic model called CityDetails using the BaseModel class. Let's break it down:**\n",
    "\n",
    "**Class Definition:**\n",
    "\n",
    "- CityDetails inherits from BaseModel, making it a data validation model.\n",
    "\n",
    "- Fields with Type Annotations:\n",
    "\n",
    "- state_name: A string representing the state name of a city.\n",
    "\n",
    "- state_capital: A string representing the capital of that state.\n",
    "\n",
    "- country_name: A string representing the country name where the city is located.\n",
    "\n",
    "- country_capital: A string representing the capital of that country.\n",
    "\n",
    "**Field Descriptions:**\n",
    "\n",
    "Each field uses Field() to provide a description that helps clarify what information each attribute represents.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "The model validates and structures data related to cities.\n",
    "Useful for ensuring that any data related to cities has the correct structure before processing.\n",
    "Helpful in APIs (like FastAPI) to generate self-documenting endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDetails(BaseModel):\n",
    "    \"\"\"Respond to the user with this\"\"\"\n",
    "    state_name: str = Field(description=\"State name of the city\")\n",
    "    state_capital: str = Field(description=\"State capital of the city\")\n",
    "    country_name: str = Field(description=\"Country name of the city\")\n",
    "    country_capital: str = Field(description=\"Country capital of the city\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit 'messages' key from MessagesState, which is a list of chat messages\n",
    "class AgentState(MessagesState):\n",
    "    # Final structured response from the agent\n",
    "    final_response: CityDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structured_output = llm.with_structured_output(CityDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState): \n",
    "    print(f\" this is 01 input from call model {state}\")\n",
    "    messages= state['messages']\n",
    "    response= llm_with_tools.invoke(messages)\n",
    "    print(f\"this is 02 response from call model  {response}\")\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_function(state: AgentState):\n",
    "    messages= state['messages']\n",
    "    last_message= messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return 'tools'\n",
    "    else:\n",
    "        return 'respond'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(state: AgentState):\n",
    "    print(f\"here is 03 state from respond {state}\")\n",
    "    messages= state['messages']\n",
    "    response= llm_with_structured_output.invoke([HumanMessage(content=messages[-1].content)])\n",
    "    print(f\"this is 04 response from respond{response}\")\n",
    "    return {\"final_response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LangGraph\n",
    "\n",
    "workflow= StateGraph(AgentState)\n",
    "workflow.add_node('agent', call_model)\n",
    "workflow.add_node('tools', tool_node)\n",
    "workflow.add_node('respond', respond)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges('agent', router_function, {'tools':'tools', 'respond':'respond'})\n",
    "workflow.add_edge('tools', 'agent')\n",
    "workflow.add_edge('respond', END)\n",
    "app= workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "# View\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is 01 input from call model {'messages': [HumanMessage(content='Tell me about the city details for chennai?', additional_kwargs={}, response_metadata={}, id='f7883560-c60e-41b7-ad09-f7c05eaf643a')]}\n",
      "{'name': 'call_tool', 'description': 'Should do a web search to find the required city details', 'parameters': {'type_': 6, 'description': 'Should do a web search to find the required city details', 'properties': {'prompt': {'type_': 0, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['prompt'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[prompt].type: must be specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 * GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[prompt].type: must be specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about the city details for chennai?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m answer\u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_response\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1569\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1568\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1570\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1571\u001b[0m     config,\n\u001b[0;32m   1572\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1573\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1574\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1575\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1576\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1578\u001b[0m ):\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1580\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1307\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1302\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[0;32m   1303\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1304\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1305\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1306\u001b[0m     ):\n\u001b[1;32m-> 1307\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1308\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1309\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1310\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1311\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1312\u001b[0m         ):\n\u001b[0;32m   1313\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\pregel\\runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\pregel\\retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m, in \u001b[0;36mcall_model\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this is 01 input from call model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m messages\u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m response\u001b[38;5;241m=\u001b[39m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is 02 response from call model  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: [response]}\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5358\u001b[0m     )\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:990\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    970\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    980\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    981\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    982\u001b[0m         messages,\n\u001b[0;32m    983\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    989\u001b[0m     )\n\u001b[1;32m--> 990\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    991\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    993\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m    994\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m    995\u001b[0m     )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:202\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[prompt].type: must be specified\n"
     ]
    }
   ],
   "source": [
    "response= app.invoke(input={\"messages\":[(\"human\", \"Tell me about the city details for chennai?\")]})\n",
    "answer= response['final_response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
