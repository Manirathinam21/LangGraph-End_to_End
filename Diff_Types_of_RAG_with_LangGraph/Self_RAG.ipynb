{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look into the retriever grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Generative_AI\\Projects\\LangGraph-End_to_End\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are a grader checking if a document is relevant to a user’s question.The check has to be done very strictly..  \n",
    "If the document has words or meanings related to the question, mark it as relevant.  \n",
    "Give a simple 'yes' or 'no' answer to show if the document is relevant or not.\"\"\"\n",
    "    \n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a grader checking if a document is relevant to a user’s question.The check has to be done very strictly..  \\nIf the document has words or meanings related to the question, mark it as relevant.  \\nGive a simple 'yes' or 'no' answer to show if the document is relevant or not.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, template='Retrieved document: \\n\\n {document} \\n\\n User question: {question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"what is ai agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manir\\AppData\\Local\\Temp\\ipykernel_8108\\10663643.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_txt = docs[2].page_content\n",
    "\n",
    "doc_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "print(my_retrieval_grader.invoke({\"document\":doc_txt,\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "question=\"who is sunny svaita?\"\n",
    "\n",
    "print(my_retrieval_grader.invoke({\"document\":doc_txt,\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's look into the data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An AI agent is a system where a large language model (LLM) acts as the \"brain\",  making decisions and taking actions.  It is often augmented by components like planning, memory, and tool use to handle complex tasks autonomously. \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 2121, 'total_tokens': 2175, 'completion_time': 0.098181818, 'prompt_time': 0.093427614, 'queue_time': 0.241685296, 'total_time': 0.191609432}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3bbf667c-57d0-4abe-843a-cd783bdd5ece-0', usage_metadata={'input_tokens': 2121, 'output_tokens': 54, 'total_tokens': 2175})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"what is a AI agent?\"\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are a grader checking if an LLM generation is grounded in or supported by a set of retrieved facts.  \n",
    "Give a simple 'yes' or 'no' answer. 'Yes' means the generation is grounded in or supported by a set of retrieved the facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "hallucinations_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "print(hallucinations_grader.invoke({\"documents\": docs, \"generation\": generation}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Answer Grader\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "print(answer_grader.invoke({\"question\": question, \"generation\": generation}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a question re-writer that converts an input question into a better optimized version for vector store retrieval document.  \n",
    "You are given both a question and a document.  \n",
    "- First, check if the question is relevant to the document by identifying a connection or relevance between them.  \n",
    "- If there is a little relevancy, rewrite the question based on the semantic intent of the question and the context of the document.  \n",
    "- If no relevance is found, simply return this single word \"question not relevant.\" dont return the entire phrase \n",
    "Your goal is to ensure the rewritten question aligns well with the document for better retrieval.\"\"\"\n",
    "     \n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\"\"\"Here is the initial question: \\n\\n {question} \\n,\n",
    "             Here is the document: \\n\\n {documents} \\n ,\n",
    "             Formulate an improved question. if possible other return 'question not relevant'.\"\"\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"who is a current indian prime minister?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question not relevant \\n\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter.invoke({\"question\":question,\"documents\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    filter_documents: List[str]\n",
    "    unfilter_documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state:AgentState):\n",
    "    print(\"----RETRIEVE----\")\n",
    "    question=state['question']\n",
    "    documents=retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_documents(state:AgentState):\n",
    "    print(\"----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    filtered_docs = []\n",
    "    unfiltered_docs = []\n",
    "    for doc in documents:\n",
    "        score=my_retrieval_grader.invoke({\"question\":question, \"document\":doc})\n",
    "        grade=score.binary_score\n",
    "        \n",
    "        if grade=='yes':\n",
    "            print(\"----GRADE: DOCUMENT RELEVANT----\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"----GRADE: DOCUMENT NOT RELEVANT----\")\n",
    "            unfiltered_docs.append(doc)\n",
    "    if len(unfiltered_docs)>1:\n",
    "        return {\"unfilter_documents\": unfiltered_docs,\"filter_documents\":[], \"question\": question}\n",
    "    else:\n",
    "        return {\"filter_documents\": filtered_docs,\"unfilter_documents\":[],\"question\": question}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state:AgentState):\n",
    "    print(\"----ACCESS GRADED DOCUMENTS----\")\n",
    "    state[\"question\"]\n",
    "    unfiltered_documents = state[\"unfilter_documents\"]\n",
    "    filtered_documents = state[\"filter_documents\"]\n",
    "    \n",
    "    \n",
    "    if unfiltered_documents:\n",
    "        print(\"----ALL THE DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY----\")\n",
    "        return \"transform_query\"\n",
    "    if filtered_documents:\n",
    "        print(\"----DECISION: GENERATE----\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state:AgentState):\n",
    "    print(\"----GENERATE----\")\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": documents,\"question\":question})\n",
    "    return {\"documents\":documents,\"question\":question,\"generation\":generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "def transform_query(state:AgentState):\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    \n",
    "    print(f\"this is my document{documents}\")\n",
    "    response = question_rewriter.invoke({\"question\":question,\"documents\":documents})\n",
    "    print(f\"----RESPONSE---- {response}\")\n",
    "    if response == 'question not relevant':\n",
    "        print(\"----QUESTION IS NOT AT ALL RELEVANT----\")\n",
    "        return {\"documents\":documents,\"question\":response,\"generation\":\"question was not at all relevant\"}\n",
    "    else:   \n",
    "        return {\"documents\":documents,\"question\":response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate_after_transformation(state:AgentState):\n",
    "    question=state[\"question\"]\n",
    "    \n",
    "    if question==\"question not relevant\":\n",
    "        return \"query_not_at_all_relevant\"\n",
    "    else:\n",
    "        return \"Retriever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint\n",
    "def grade_generation_vs_documents_and_question(state:AgentState):\n",
    "    print(\"---CHECK HELLUCINATIONS---\")\n",
    "    question= state['question']\n",
    "    documents = state['documents']\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    score = hallucinations_grader.invoke({\"documents\":documents,\"generation\":generation})\n",
    "    \n",
    "    grade = score.binary_score\n",
    "    \n",
    "    #Check hallucinations\n",
    "    if grade=='yes':\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        \n",
    "        print(\"---GRADE GENERATION vs QUESTION ---\")\n",
    "        \n",
    "        score = answer_grader.invoke({\"question\":question,\"generation\":generation})\n",
    "        \n",
    "        grade = score.binary_score\n",
    "        \n",
    "        if grade=='yes':\n",
    "            print(\"---DECISION: GENERATION ADDRESS THE QUESTION ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---TRANSFORM QUERY\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---TRANSFORM QUERY\")\n",
    "        \"not useful\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here the Langgraph workflow will start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1a6b08ccaf0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Docs_Vector_Retrieve\", retrieve)\n",
    "workflow.add_node(\"Grading_Generated_Documents\", grade_documents) \n",
    "workflow.add_node(\"Content_Generator\", generate)\n",
    "workflow.add_node(\"Transform_User_Query\", transform_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(START,\"Docs_Vector_Retrieve\")\n",
    "workflow.add_edge(\"Docs_Vector_Retrieve\",\"Grading_Generated_Documents\")\n",
    "workflow.add_conditional_edges(\"Grading_Generated_Documents\",\n",
    "                            decide_to_generate,\n",
    "                            {\n",
    "                            \"generate\": \"Content_Generator\",\n",
    "                            \"transform_query\": \"Transform_User_Query\"\n",
    "                            }\n",
    "                            )\n",
    "workflow.add_conditional_edges(\"Content_Generator\",\n",
    "                            grade_generation_vs_documents_and_question,\n",
    "                            {\n",
    "                            \"useful\": END,\n",
    "                            \"not useful\": \"Transform_User_Query\",\n",
    "                            }\n",
    "                            )\n",
    "workflow.add_conditional_edges(\"Transform_User_Query\",\n",
    "                decide_to_generate_after_transformation,\n",
    "                {\n",
    "                \"Retriever\":\"Docs_Vector_Retrieve\",\n",
    "                \"query_not_at_all_relevant\":END\n",
    "                }\n",
    "                )\n",
    "\n",
    "app=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display # type: ignore\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----DECISION: GENERATE----\n",
      "----GENERATE----\n",
      "---CHECK HELLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION ---\n",
      "---DECISION: GENERATION ADDRESS THE QUESTION ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLM-powered agents use short-term memory for in-context learning and long-term memory to store and recall information over extended periods.  Long-term memory often relies on external vector stores for efficient retrieval. \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 2126, 'total_tokens': 2174, 'completion_time': 0.087272727, 'prompt_time': 0.07268995, 'queue_time': 0.24012048200000002, 'total_time': 0.159962677}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1aae0119-3cdb-4022-8ac8-266926540a5d-0', usage_metadata={'input_tokens': 2126, 'output_tokens': 48, 'total_tokens': 2174})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "app.invoke(inputs)[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----DECISION: GENERATE----\n",
      "----GENERATE----\n",
      "---CHECK HELLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION ---\n",
      "---DECISION: GENERATION ADDRESS THE QUESTION ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Prompt engineering is the art of designing effective inputs (prompts) for large language models (LLMs) to elicit desired responses.  It involves carefully crafting the prompt to guide the LLM's behavior and achieve specific outcomes without altering the model's weights.  Prompt engineering is a key technique for controlling and steering the output of LLMs. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1107, 'total_tokens': 1181, 'completion_time': 0.134545455, 'prompt_time': 0.03919777, 'queue_time': 0.236210989, 'total_time': 0.173743225}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-23147476-6030-4550-b46b-ef4865dd4d35-0', usage_metadata={'input_tokens': 1107, 'output_tokens': 74, 'total_tokens': 1181})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"who is a prompt engineering?\"}\n",
    "app.invoke(inputs)[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----ALL THE DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY----\n",
      "this is my document[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='},\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Tips for Example Selection#'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Few-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.')]\n",
      "----RESPONSE---- who was the first president of the united states? \n",
      "\n",
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----ALL THE DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY----\n",
      "this is my document[Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Definition: Determine which category the question asks for, \"Quantity\" or \"Location\".\\nInput: What\\'s the oldest building in US?\\nOuput: Location'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='It takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.\\nSo the answer is 2.\\n===\\nQuestion: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Answer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.\\nThe total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.\\nJack need $111 - $40 = $<<111-40=71>>71 more.\\nSo the answer is 71.\\n===')]\n",
      "----RESPONSE---- question not relevant \n",
      "\n",
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----ALL THE DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY----\n",
      "this is my document[Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Later, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.\\n\\n\\nChanging Q: to Question: is found to be helpful. (Fu et al. 2023)'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='One observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='For closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\\nEvidence: ...\\nQuestion: ...\\nAnswer: ...\\nThe answer probability is computed in three ways:')]\n",
      "----RESPONSE---- Is prompt swapping (longer distance between question and answer) generally effective for closed-book QA? \n",
      "\n",
      "\n",
      "\n",
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT NOT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----DECISION: GENERATE----\n",
      "----GENERATE----\n",
      "---CHECK HELLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION ---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---TRANSFORM QUERY\n",
      "this is my document[Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='For closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\\nEvidence: ...\\nQuestion: ...\\nAnswer: ...\\nThe answer probability is computed in three ways:'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Later, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.\\n\\n\\nChanging Q: to Question: is found to be helpful. (Fu et al. 2023)'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='and $p_\\\\text{LM}(q \\\\mid p_i, a)$ are found to be most informative. $p_\\\\text{LM}(q \\\\mid p_i, a)$ captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.')]\n",
      "----RESPONSE---- Does changing the position of the question relative to the evidence in a few-shot prompt affect closed-book QA performance? \n",
      "\n",
      "----RETRIEVE----\n",
      "----CHECK DOCUMENTS RELEVANCE TO THE QUESTION----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----GRADE: DOCUMENT RELEVANT----\n",
      "----ACCESS GRADED DOCUMENTS----\n",
      "----DECISION: GENERATE----\n",
      "----GENERATE----\n",
      "---CHECK HELLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION ---\n",
      "---DECISION: GENERATION ADDRESS THE QUESTION ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, changing the position of the question relative to the evidence can affect closed-book QA performance.  Swapping the question with the evidence (longer distance) consistently yields lower results across datasets.  \\n\\nThis is according to research cited in the provided text. \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 1107, 'total_tokens': 1163, 'completion_time': 0.101818182, 'prompt_time': 0.038735758, 'queue_time': 0.23418026999999997, 'total_time': 0.14055394}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-93518266-acee-4149-9005-1980112cfe73-0', usage_metadata={'input_tokens': 1107, 'output_tokens': 56, 'total_tokens': 1163})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = {\"question\": \"who is a first president of USA?\"}\n",
    "app.invoke(inputs)[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
